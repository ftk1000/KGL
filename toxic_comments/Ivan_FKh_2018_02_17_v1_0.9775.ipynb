{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# LSTM + GloVe + Cross-validation + LearningRate changes + ...\n",
    "https://www.kaggle.com/demesgal/lstm-glove-lr-decrease-bn-cv-lb-0-047/notebook\n",
    "# Notes\n",
    "1) GRU is very similar to LSTM and not better\n",
    "\n",
    "2) GloVe dimension is very important. I recommend to use GloVe 840b 300d if you can (it's very hard to use it in kaggle kernels)\n",
    "\n",
    "3) Cross Validation is interesting for hiperparameters tuning, but for higher score you shoudn't use validation_split\n",
    "\n",
    "4) First Epoch is very unstable. So I use small LR on first step\n",
    "\n",
    "5) Dataset size is small. So you may use some additional datasets and then finetune model\n",
    "\n",
    "6) It's hard not to overfit the model and I have n't found yet a good way to solve this problem. BatchNormalization/Dropout don't really help. \n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46494#263247\n",
    "Maybe Reccurent Batch Normalization can help, but it is'not implemented in keras.\n",
    "\n",
    "7) Use Attention layer from here (AttLayer):\n",
    "https://github.com/dem-esgal/textClassifier/blob/master/textClassifierHATT.py\n",
    "\n",
    "Thanks to (https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "Glove dimension 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [],
   "source": [
    "#path = '../input/'\n",
    "path ='C:/__KAGGLE__/ToxicCommentClassification'\n",
    "#comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "\n",
    "#-----------------------------------\n",
    "# http://ronny.rest/blog/post_2017_08_04_glove/\n",
    "#-----------------------------------\n",
    "#EMBEDDING_FILE=f'{path}glove-vectors/glove.6B.100d.txt'\n",
    "#EMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n",
    "EMBEDDING_FILE=f'{path}/glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "#TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "#TEST_DATA_FILE=f'{path}{comp}test.csv'\n",
    "TRAIN_DATA_FILE=f'{path}/data_2018_02_16/train.csv/train.csv'\n",
    "TEST_DATA_FILE=f'{path}/data_2018_02_16/test.csv/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape # (159571, 8)\n",
    "type(train) # pandas.core.frame.DataFrame\n",
    "train.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape # (153164, 2)\n",
    "test.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)  # max_features = 20000 = how many unique words to use\n",
    "tokenizer.fit_on_texts(list(list_sentences_train)) # fit is done on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of train and test sentences\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t[0,:]= [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0   688    75     1   126   130   177    29\n",
      "   672  4511 12052  1116    86   331    51  2278 11448    50  6864    15\n",
      "    60  2756   148     7  2937    34   117  1221 15190  2825     4    45\n",
      "    59   244     1   365    31     1    38    27   143    73  3462    89\n",
      "  3085  4583  2273   985] \n",
      "    X_te[0,]= [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  2665   655  8849   656\n",
      "     8    57 16388    83   884   356    16  3222    76    21     6     4\n",
      "  6865     6  1521     7    56   655  4942  1898   682  6908     4    96\n",
      "     6     2  5104    29   417     6   726    35  8849   656     8    36\n",
      "  4122    10  2818   660   437   454 19612     9   333    15   153     4\n",
      "     8   240    49    52    24     5  2045   162  3132   682  2880    96\n",
      "   219   145   493    84]\n"
     ]
    }
   ],
   "source": [
    "type(list_tokenized_train) # list\n",
    "#list_tokenized_train[0:4]\n",
    "type(X_t) #numpy.ndarray\n",
    "X_t.shape # (159571, 100)\n",
    "print(\"X_t[0,:]=\",X_t[0,:], \"\\n    X_te[0,]=\", X_te[0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Bidirectional LSTM with half-size embedding with two fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "DropOutRate = 0.1 #0.1\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=DropOutRate, recurrent_dropout=DropOutRate))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dropout(DropOutRate)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "import keras.backend as K\n",
    "def loss(y_true, y_pred):\n",
    "     return K.binary_crossentropy(y_true, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          40400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 1,046,156\n",
      "Trainable params: 1,045,956\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when for hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 561s 4ms/step - loss: 0.0623 - acc: 0.8798 - val_loss: 0.0486 - val_acc: 0.9934\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 564s 4ms/step - loss: 0.0470 - acc: 0.9507 - val_loss: 0.0476 - val_acc: 0.9920\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 561s 4ms/step - loss: 0.0414 - acc: 0.9880 - val_loss: 0.0474 - val_acc: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5829c8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def schedule(ind):\n",
    "    a = [0.002,0.003, 0.000]\n",
    "    return a[ind]\n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "model.fit(X_t, y, batch_size=64, epochs=3, validation_split=0.1, callbacks=[lr])\n",
    "\n",
    "#model.fit(X_t, y, batch_size=512, epochs=1, validation_split=0.1)\n",
    "#model.fit(X_t, y, batch_size=64, epochs=3, callbacks=[lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 122s 797us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission = pd.read_csv(f'{path}/data_2018_02_16/sample_submission.csv/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(f'{path}/Ivan_0110/2018_02_16/v1/LSTM-submission_2018_02_17_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [],
   "source": [
    "ivan_mod_2018_02_17_config_v1 = model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(f'{path}/data_2018_02_16/model_2018_02_16_v2.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(f'{path}/data_2018_02_16/model_2018_02_16_v2.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

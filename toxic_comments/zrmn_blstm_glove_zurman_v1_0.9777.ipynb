{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "* # B-LSTM + GloVe \n",
    "Tahnks to https://www.kaggle.com/demesgal/lstm-glove-lr-decrease-bn-cv-lb-0-047/notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "Glove dimension 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [],
   "source": [
    "#path = '../input/'\n",
    "#comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'../glove.6B/glove.6B.100d.txt'\n",
    "#EMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n",
    "\n",
    "#TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "#TEST_DATA_FILE=f'{path}{comp}test.csv'\n",
    "path='../data_2018_02_16/'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv/train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "\n",
    "maxlen = 100 # max number of words in a comment to use\n",
    "NEPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "#train = pd.read_csv(TRAIN_DATA_FILE , nrows=1024 )\n",
    "#test = pd.read_csv(TEST_DATA_FILE, nrows=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "9ad0cfa645c82567f101c891e72af69c2d1f0c60"
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
    "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
    "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.004451992, 0.40815741)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7db5a31-fa22-4468-8906-e154d25a9567",
    "_uuid": "a157169c19df565ce14cb530af5241660d699987"
   },
   "source": [
    "ROC AUC for CV in Keras see for details: https://gist.github.com/smly/d29d079100f8d81b905e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7c3e4fc2-3a00-4387-ae3a-f0584411dc2d",
    "_uuid": "dee46714c5b683bf0e15c792fed76ae0a5289c10"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "scores = []\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "            scores.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "Bidirectional LSTM with half-size embedding with two fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "DROPOUT_VAL = 0.1\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "#x = Bidirectional(LSTM(50, return_sequences=True,dropout=DROPOUT_VAL, recurrent_dropout=DROPOUT_VAL))(x)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True,dropout=DROPOUT_VAL, recurrent_dropout=DROPOUT_VAL))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_VAL)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "import keras.backend as K\n",
    "def loss(y_true, y_pred):\n",
    "     return K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "model.compile(loss=loss, optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4a624b55-3720-42bc-ad5a-7cefc76d83f6",
    "_uuid": "e2a0e9ce12e1ff5ea102665e79de23df5caf5802"
   },
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when for hyperparams tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "77b296a5-caef-4461-a42f-a3845bbfb418",
    "_uuid": "624936a672d5a691e213af329433c29dae4abd23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,066,156\n",
      "Trainable params: 2,065,956\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\farid khafizov\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/3\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.7845\n",
      " ROC-AUC - epoch: 0 - score: 0.979342\n",
      "151592/151592 [==============================] - 753s 5ms/step - loss: 0.0710 - acc: 0.7846 - val_loss: 0.0520 - val_acc: 0.9876\n",
      "Epoch 2/3\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9512\n",
      " ROC-AUC - epoch: 1 - score: 0.984027\n",
      "151592/151592 [==============================] - 823s 5ms/step - loss: 0.0479 - acc: 0.9512 - val_loss: 0.0462 - val_acc: 0.9921\n",
      "Epoch 3/3\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9495\n",
      " ROC-AUC - epoch: 2 - score: 0.983355\n",
      "151592/151592 [==============================] - 798s 5ms/step - loss: 0.0432 - acc: 0.9495 - val_loss: 0.0473 - val_acc: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5df01f28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def schedule(ind):\n",
    "    a = [0.001,0.002,0.003, 0.000]\n",
    "    return a[ind]\n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "[X_train, X_val, y_train, y_val] = train_test_split(X_t, y, train_size=0.95)\n",
    "\n",
    "ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=NEPOCHS, \n",
    "          validation_data=(X_val, y_val), callbacks=[lr, ra_val])\n",
    "#model.fit(X_t, y, batch_size=64, epochs=3, callbacks=[lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
    "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
   },
   "source": [
    "And finally, get predictions for the test set and prepare a submission CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 158s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "y_test = model.predict([X_te], batch_size=64, verbose=1)\n",
    "#sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv/sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4c67602eb98257c13b1f4edb1fec8a1638c16642"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv('../Zurman/subm_ep3_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97934176731361322, 0.98402680073390247, 0.98335544649007545]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "slen=len(scores)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(all_loss)\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('loss values')\n",
    "plt.ylabel('all losses')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(all_loss[histlen-5:histlen])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('LAST 5 loss values')\n",
    "plt.ylabel('last 5 losses')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "type(scores)\n",
    "len(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

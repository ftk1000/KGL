# https://www.kaggle.com/c/lish-moa

[2020.11 Андрей Лукьяненко : "Обзор текущих подходов в соревновании "Mechanisms of Action (MoA) Prediction" на Kaggle" ](https://youtu.be/D7i67UT3O3o)<br>
[2020.09 Matthew Masters: Insight into the MoA Pred Comp](https://www.kaggle.com/c/lish-moa/discussion/184005)<br>

[2020.09 Shahules discusses: Best Single model - 1st month](https://www.kaggle.com/c/lish-moa/discussion/188624)<br>
[2020.10 mavillan discusses: Best Single model - 2nd month](https://www.kaggle.com/c/lish-moa/discussion/193907)<br>
[]()<br>
[]()<br>
[]()<br>



[make-final-submission-the-efficient-way   Python notebook using data from multiple data sources · 10,562 views  ](https://www.kaggle.com/underwearfitting/make-final-submission-the-efficient-way/comments)<br>

# https://www.kaggle.com/c/lish-moa/notebooks?competitionId=19988&searchQuery=alexandervc

[MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual, AKA PUBLIC BASELINE](https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual)<br>

[Sasha's notebooks](https://www.kaggle.com/c/lish-moa/notebooks?competitionId=19988&searchQuery=alexandervc)<br>

[Alexandervc: moa-data-visualization-via-dimensional-reduc](https://www.kaggle.com/alexandervc/moa-data-visualization-via-dimensional-reduct)<br>
[Alexandervc: 
MoA Mysterious Patterns for LocallyLinearEmbedding](https://www.kaggle.com/alexandervc/moa-mysterious-patterns-for-locallylinearembedding)<br>
[]()<br>
[]()<br>


[https://www.kaggle.com/alexandervc/moa-correlation-analysis-use-igraph](https://www.kaggle.com/alexandervc/moa-correlation-analysis-use-igraph)<br>
[]()<br>
[Grandmaster Ahmet Erdem https://www.kaggle.com/aerdem4/moa-xgb-svm-solution](https://www.kaggle.com/zurman/moa-xgb-svm-solution/edit)<br>

    Congrats everyone who won medals and who learned new things. I have noticed this competition didn't have any strong non-NN baseline. I am sharing a short and clean notebook which has 0.01876 public, 0.01659 private score. It is not very competitive but it is diverse enough to improve ensembles. Since it runs in 23 minutes including training, I expect an inference only run to be superfast. They both run on GPU. You can easily add this to your ensembles. I did some tricks to mimic label smoothing in XGB and SVM. They improved their scores significantly.    https://www.kaggle.com/aerdem4/moa-xgb-svm-solution

    

[]()<br>
# NN for MoA<br>
==> from tensorflow.keras.utils import plot_model<br>
[https://www.kaggle.com/damoonshahhosseini/aggregated-neural-networks -**from tensorflow.keras.utils import plot_model** I forked it](https://www.kaggle.com/damoonshahhosseini/aggregated-neural-networks)<br>
[Alexnader D'yakonov's kernel moa-nn-04](https://www.kaggle.com/zurman/moa-nn-04)<br>

### https://pypi.org/project/keras-one-cycle-lr/
[DL TOOL: keras-one-cycle-lr](https://pypi.org/project/keras-one-cycle-lr/)<br>
[Novichok's MLP + OneCycleLR + PseudoLabeling  <- uses https://pypi.org/project/keras-one-cycle-lr/ ](https://www.kaggle.com/alturutin/mlp-onecyclelr-pseudolabeling)<br>
[]()<br>
[]()<br>
[]()<br>
[]()<br>



